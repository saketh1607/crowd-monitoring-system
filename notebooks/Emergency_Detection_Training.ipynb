{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "emergency_detection_title"
   },
   "source": [
    "# Emergency Detection Model Training\n",
    "## Large Events Emergency Management System\n",
    "\n",
    "This notebook trains ML models for detecting emergencies (fire, medical, security) using Google Colab's GPU resources.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup_section"
   },
   "source": [
    "## 1. Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_dependencies"
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install tensorflow==2.13.0\n",
    "!pip install torch torchvision\n",
    "!pip install opencv-python\n",
    "!pip install scikit-learn\n",
    "!pip install pandas numpy matplotlib seaborn\n",
    "!pip install pillow\n",
    "!pip install tqdm\n",
    "\n",
    "# Check GPU availability\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"GPU available (TF): {tf.config.list_physical_devices('GPU')}\")\n",
    "print(f\"GPU available (PyTorch): {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import_libraries"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# TensorFlow/Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, optimizers, callbacks\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import VGG16, ResNet50\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, IsolationForest\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "data_section"
   },
   "source": [
    "## 2. Data Preparation and Synthetic Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_synthetic_data"
   },
   "outputs": [],
   "source": [
    "# Create synthetic training data for demonstration\n",
    "def generate_synthetic_fire_data(num_samples=1000):\n",
    "    \"\"\"Generate synthetic fire detection training data\"\"\"\n",
    "    # Simulate image features (in real scenario, use actual fire/non-fire images)\n",
    "    data = []\n",
    "    labels = []\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        if i < num_samples // 2:\n",
    "            # Fire images - higher red/orange values, specific patterns\n",
    "            red_intensity = np.random.normal(200, 30)\n",
    "            orange_intensity = np.random.normal(180, 25)\n",
    "            brightness = np.random.normal(150, 20)\n",
    "            motion_intensity = np.random.normal(80, 15)\n",
    "            label = 1  # Fire\n",
    "        else:\n",
    "            # Non-fire images\n",
    "            red_intensity = np.random.normal(100, 40)\n",
    "            orange_intensity = np.random.normal(90, 35)\n",
    "            brightness = np.random.normal(120, 30)\n",
    "            motion_intensity = np.random.normal(30, 20)\n",
    "            label = 0  # No fire\n",
    "        \n",
    "        # Create feature vector\n",
    "        features = [\n",
    "            red_intensity, orange_intensity, brightness, motion_intensity,\n",
    "            np.random.normal(50, 10),  # Additional features\n",
    "            np.random.normal(75, 15)\n",
    "        ]\n",
    "        \n",
    "        data.append(features)\n",
    "        labels.append(label)\n",
    "    \n",
    "    return np.array(data), np.array(labels)\n",
    "\n",
    "def generate_synthetic_crowd_data(num_samples=1000):\n",
    "    \"\"\"Generate synthetic crowd density data\"\"\"\n",
    "    data = []\n",
    "    labels = []\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        # Simulate crowd features\n",
    "        people_count = np.random.randint(0, 200)\n",
    "        area_coverage = np.random.uniform(0.1, 0.9)\n",
    "        movement_speed = np.random.uniform(0, 5)\n",
    "        noise_level = np.random.uniform(40, 100)\n",
    "        \n",
    "        # Calculate density level\n",
    "        density = people_count * area_coverage / 100\n",
    "        if density < 1.0:\n",
    "            density_level = 0  # Low\n",
    "        elif density < 2.5:\n",
    "            density_level = 1  # Medium\n",
    "        elif density < 4.0:\n",
    "            density_level = 2  # High\n",
    "        else:\n",
    "            density_level = 3  # Critical\n",
    "        \n",
    "        features = [people_count, area_coverage, movement_speed, noise_level, density]\n",
    "        data.append(features)\n",
    "        labels.append(density_level)\n",
    "    \n",
    "    return np.array(data), np.array(labels)\n",
    "\n",
    "def generate_synthetic_behavior_data(num_samples=1000):\n",
    "    \"\"\"Generate synthetic behavior analysis data\"\"\"\n",
    "    data = []\n",
    "    labels = []\n",
    "    \n",
    "    behavior_types = ['normal', 'suspicious', 'aggressive', 'panic']\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        behavior = np.random.choice(behavior_types)\n",
    "        \n",
    "        if behavior == 'normal':\n",
    "            motion_variance = np.random.normal(10, 3)\n",
    "            audio_level = np.random.normal(60, 10)\n",
    "            speed_changes = np.random.normal(2, 1)\n",
    "        elif behavior == 'suspicious':\n",
    "            motion_variance = np.random.normal(25, 5)\n",
    "            audio_level = np.random.normal(50, 8)\n",
    "            speed_changes = np.random.normal(8, 2)\n",
    "        elif behavior == 'aggressive':\n",
    "            motion_variance = np.random.normal(40, 8)\n",
    "            audio_level = np.random.normal(85, 15)\n",
    "            speed_changes = np.random.normal(15, 3)\n",
    "        else:  # panic\n",
    "            motion_variance = np.random.normal(60, 10)\n",
    "            audio_level = np.random.normal(95, 20)\n",
    "            speed_changes = np.random.normal(25, 5)\n",
    "        \n",
    "        features = [\n",
    "            motion_variance, audio_level, speed_changes,\n",
    "            np.random.normal(30, 10),  # Additional features\n",
    "            np.random.normal(45, 12)\n",
    "        ]\n",
    "        \n",
    "        data.append(features)\n",
    "        labels.append(behavior)\n",
    "    \n",
    "    return np.array(data), np.array(labels)\n",
    "\n",
    "# Generate synthetic datasets\n",
    "print(\"Generating synthetic training data...\")\n",
    "fire_data, fire_labels = generate_synthetic_fire_data(2000)\n",
    "crowd_data, crowd_labels = generate_synthetic_crowd_data(2000)\n",
    "behavior_data, behavior_labels = generate_synthetic_behavior_data(2000)\n",
    "\n",
    "print(f\"Fire detection data: {fire_data.shape}, Labels: {fire_labels.shape}\")\n",
    "print(f\"Crowd analysis data: {crowd_data.shape}, Labels: {crowd_labels.shape}\")\n",
    "print(f\"Behavior analysis data: {behavior_data.shape}, Labels: {behavior_labels.shape}\")\n",
    "print(\"Synthetic data generated successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fire_detection_section"
   },
   "source": [
    "## 3. Fire Detection Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train_fire_model"
   },
   "outputs": [],
   "source": [
    "# Prepare fire detection data\n",
    "X_fire_train, X_fire_test, y_fire_train, y_fire_test = train_test_split(\n",
    "    fire_data, fire_labels, test_size=0.2, random_state=42, stratify=fire_labels\n",
    ")\n",
    "\n",
    "# Scale features\n",
    "fire_scaler = StandardScaler()\n",
    "X_fire_train_scaled = fire_scaler.fit_transform(X_fire_train)\n",
    "X_fire_test_scaled = fire_scaler.transform(X_fire_test)\n",
    "\n",
    "# Create and train fire detection model\n",
    "def create_fire_detection_model(input_dim):\n",
    "    model = models.Sequential([\n",
    "        layers.Dense(128, activation='relu', input_shape=(input_dim,)),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy', 'precision', 'recall']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Train fire detection model\n",
    "print(\"Training fire detection model...\")\n",
    "fire_model = create_fire_detection_model(X_fire_train_scaled.shape[1])\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = callbacks.EarlyStopping(\n",
    "    monitor='val_loss', patience=10, restore_best_weights=True\n",
    ")\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss', factor=0.2, patience=5, min_lr=0.0001\n",
    ")\n",
    "\n",
    "# Train model\n",
    "fire_history = fire_model.fit(\n",
    "    X_fire_train_scaled, y_fire_train,\n",
    "    validation_data=(X_fire_test_scaled, y_fire_test),\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stopping, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate model\n",
    "fire_predictions = (fire_model.predict(X_fire_test_scaled) > 0.5).astype(int)\n",
    "fire_accuracy = accuracy_score(y_fire_test, fire_predictions)\n",
    "\n",
    "print(f\"\\nFire Detection Model Accuracy: {fire_accuracy:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_fire_test, fire_predictions))\n",
    "\n",
    "# Save model\n",
    "fire_model.save('/content/fire_detection_model.h5')\n",
    "print(\"Fire detection model saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "crowd_analysis_section"
   },
   "source": [
    "## 4. Crowd Density Analysis Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train_crowd_model"
   },
   "outputs": [],
   "source": [
    "# Prepare crowd analysis data\n",
    "X_crowd_train, X_crowd_test, y_crowd_train, y_crowd_test = train_test_split(\n",
    "    crowd_data, crowd_labels, test_size=0.2, random_state=42, stratify=crowd_labels\n",
    ")\n",
    "\n",
    "# Scale features\n",
    "crowd_scaler = StandardScaler()\n",
    "X_crowd_train_scaled = crowd_scaler.fit_transform(X_crowd_train)\n",
    "X_crowd_test_scaled = crowd_scaler.transform(X_crowd_test)\n",
    "\n",
    "# Create and train crowd density model\n",
    "def create_crowd_density_model(input_dim, num_classes):\n",
    "    model = models.Sequential([\n",
    "        layers.Dense(128, activation='relu', input_shape=(input_dim,)),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Train crowd density model\n",
    "print(\"Training crowd density analysis model...\")\n",
    "num_density_classes = len(np.unique(crowd_labels))\n",
    "crowd_model = create_crowd_density_model(X_crowd_train_scaled.shape[1], num_density_classes)\n",
    "\n",
    "# Train model\n",
    "crowd_history = crowd_model.fit(\n",
    "    X_crowd_train_scaled, y_crowd_train,\n",
    "    validation_data=(X_crowd_test_scaled, y_crowd_test),\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stopping, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate model\n",
    "crowd_predictions = np.argmax(crowd_model.predict(X_crowd_test_scaled), axis=1)\n",
    "crowd_accuracy = accuracy_score(y_crowd_test, crowd_predictions)\n",
    "\n",
    "print(f\"\\nCrowd Density Model Accuracy: {crowd_accuracy:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_crowd_test, crowd_predictions))\n",
    "\n",
    "# Save model\n",
    "crowd_model.save('/content/crowd_density_model.h5')\n",
    "print(\"Crowd density model saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "behavior_analysis_section"
   },
   "source": [
    "## 5. Behavior Analysis Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train_behavior_model"
   },
   "outputs": [],
   "source": [
    "# Prepare behavior analysis data\n",
    "# Encode labels\n",
    "behavior_encoder = LabelEncoder()\n",
    "behavior_labels_encoded = behavior_encoder.fit_transform(behavior_labels)\n",
    "\n",
    "X_behavior_train, X_behavior_test, y_behavior_train, y_behavior_test = train_test_split(\n",
    "    behavior_data, behavior_labels_encoded, test_size=0.2, random_state=42, stratify=behavior_labels_encoded\n",
    ")\n",
    "\n",
    "# Scale features\n",
    "behavior_scaler = StandardScaler()\n",
    "X_behavior_train_scaled = behavior_scaler.fit_transform(X_behavior_train)\n",
    "X_behavior_test_scaled = behavior_scaler.transform(X_behavior_test)\n",
    "\n",
    "# Train Random Forest for behavior analysis\n",
    "print(\"Training behavior analysis model...\")\n",
    "behavior_model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "behavior_model.fit(X_behavior_train_scaled, y_behavior_train)\n",
    "\n",
    "# Evaluate model\n",
    "behavior_predictions = behavior_model.predict(X_behavior_test_scaled)\n",
    "behavior_accuracy = accuracy_score(y_behavior_test, behavior_predictions)\n",
    "\n",
    "print(f\"\\nBehavior Analysis Model Accuracy: {behavior_accuracy:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(\n",
    "    y_behavior_test, \n",
    "    behavior_predictions,\n",
    "    target_names=behavior_encoder.classes_\n",
    "))\n",
    "\n",
    "# Feature importance\n",
    "feature_names = ['Motion Variance', 'Audio Level', 'Speed Changes', 'Feature 4', 'Feature 5']\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': behavior_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nFeature Importance:\")\n",
    "print(importance_df)\n",
    "\n",
    "# Save model and encoders\n",
    "import joblib\n",
    "joblib.dump(behavior_model, '/content/behavior_analysis_model.pkl')\n",
    "joblib.dump(behavior_encoder, '/content/behavior_label_encoder.pkl')\n",
    "joblib.dump(behavior_scaler, '/content/behavior_scaler.pkl')\n",
    "print(\"Behavior analysis model saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "anomaly_detection_section"
   },
   "source": [
    "## 6. Sensor Anomaly Detection Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train_anomaly_model"
   },
   "outputs": [],
   "source": [
    "# Generate synthetic sensor data\n",
    "def generate_sensor_data(num_samples=2000):\n",
    "    \"\"\"Generate synthetic sensor readings\"\"\"\n",
    "    data = []\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        # Normal readings (90% of data)\n",
    "        if i < num_samples * 0.9:\n",
    "            temperature = np.random.normal(22, 3)  # Normal room temperature\n",
    "            humidity = np.random.normal(45, 8)     # Normal humidity\n",
    "            sound = np.random.normal(65, 10)       # Normal sound level\n",
    "            motion = np.random.normal(20, 5)       # Normal motion\n",
    "        else:\n",
    "            # Anomalous readings (10% of data)\n",
    "            temperature = np.random.choice([\n",
    "                np.random.normal(45, 5),   # High temperature (fire)\n",
    "                np.random.normal(-5, 2)    # Very low temperature\n",
    "            ])\n",
    "            humidity = np.random.normal(85, 10)    # High humidity\n",
    "            sound = np.random.normal(110, 15)      # High sound (emergency)\n",
    "            motion = np.random.normal(80, 20)      # High motion (panic)\n",
    "        \n",
    "        data.append([temperature, humidity, sound, motion])\n",
    "    \n",
    "    return np.array(data)\n",
    "\n",
    "# Generate sensor data\n",
    "sensor_data = generate_sensor_data(2000)\n",
    "\n",
    "# Train anomaly detection models for each sensor type\n",
    "sensor_types = ['temperature', 'humidity', 'sound', 'motion']\n",
    "anomaly_models = {}\n",
    "sensor_scalers = {}\n",
    "\n",
    "for i, sensor_type in enumerate(sensor_types):\n",
    "    print(f\"Training anomaly detector for {sensor_type}...\")\n",
    "    \n",
    "    # Extract sensor-specific data\n",
    "    sensor_values = sensor_data[:, i].reshape(-1, 1)\n",
    "    \n",
    "    # Use only normal data for training (first 90%)\n",
    "    normal_data = sensor_values[:int(len(sensor_values) * 0.9)]\n",
    "    \n",
    "    # Scale data\n",
    "    scaler = StandardScaler()\n",
    "    normal_data_scaled = scaler.fit_transform(normal_data)\n",
    "    \n",
    "    # Train Isolation Forest\n",
    "    anomaly_model = IsolationForest(\n",
    "        contamination=0.1,\n",
    "        random_state=42,\n",
    "        n_estimators=100\n",
    "    )\n",
    "    anomaly_model.fit(normal_data_scaled)\n",
    "    \n",
    "    # Test on all data\n",
    "    all_data_scaled = scaler.transform(sensor_values)\n",
    "    predictions = anomaly_model.predict(all_data_scaled)\n",
    "    anomaly_scores = anomaly_model.decision_function(all_data_scaled)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    true_anomalies = np.concatenate([\n",
    "        np.ones(int(len(sensor_values) * 0.9)),   # Normal = 1\n",
    "        np.full(int(len(sensor_values) * 0.1), -1) # Anomaly = -1\n",
    "    ])\n",
    "    \n",
    "    accuracy = accuracy_score(true_anomalies, predictions)\n",
    "    print(f\"{sensor_type} anomaly detection accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    # Store models\n",
    "    anomaly_models[sensor_type] = anomaly_model\n",
    "    sensor_scalers[sensor_type] = scaler\n",
    "\n",
    "# Save anomaly detection models\n",
    "for sensor_type in sensor_types:\n",
    "    joblib.dump(anomaly_models[sensor_type], f'/content/{sensor_type}_anomaly_model.pkl')\n",
    "    joblib.dump(sensor_scalers[sensor_type], f'/content/{sensor_type}_scaler.pkl')\n",
    "\n",
    "print(\"\\nAll anomaly detection models saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "visualization_section"
   },
   "source": [
    "## 7. Model Performance Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "visualize_results"
   },
   "outputs": [],
   "source": [
    "# Plot training histories\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Fire detection model history\n",
    "axes[0, 0].plot(fire_history.history['accuracy'], label='Training Accuracy')\n",
    "axes[0, 0].plot(fire_history.history['val_accuracy'], label='Validation Accuracy')\n",
    "axes[0, 0].set_title('Fire Detection Model - Accuracy')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Accuracy')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True)\n",
    "\n",
    "axes[0, 1].plot(fire_history.history['loss'], label='Training Loss')\n",
    "axes[0, 1].plot(fire_history.history['val_loss'], label='Validation Loss')\n",
    "axes[0, 1].set_title('Fire Detection Model - Loss')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Loss')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True)\n",
    "\n",
    "# Crowd density model history\n",
    "axes[1, 0].plot(crowd_history.history['accuracy'], label='Training Accuracy')\n",
    "axes[1, 0].plot(crowd_history.history['val_accuracy'], label='Validation Accuracy')\n",
    "axes[1, 0].set_title('Crowd Density Model - Accuracy')\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('Accuracy')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True)\n",
    "\n",
    "axes[1, 1].plot(crowd_history.history['loss'], label='Training Loss')\n",
    "axes[1, 1].plot(crowd_history.history['val_loss'], label='Validation Loss')\n",
    "axes[1, 1].set_title('Crowd Density Model - Loss')\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].set_ylabel('Loss')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/content/model_training_history.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Confusion matrices\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Fire detection confusion matrix\n",
    "fire_cm = confusion_matrix(y_fire_test, fire_predictions)\n",
    "sns.heatmap(fire_cm, annot=True, fmt='d', cmap='Blues', ax=axes[0])\n",
    "axes[0].set_title('Fire Detection - Confusion Matrix')\n",
    "axes[0].set_xlabel('Predicted')\n",
    "axes[0].set_ylabel('Actual')\n",
    "\n",
    "# Crowd density confusion matrix\n",
    "crowd_cm = confusion_matrix(y_crowd_test, crowd_predictions)\n",
    "sns.heatmap(crowd_cm, annot=True, fmt='d', cmap='Greens', ax=axes[1])\n",
    "axes[1].set_title('Crowd Density - Confusion Matrix')\n",
    "axes[1].set_xlabel('Predicted')\n",
    "axes[1].set_ylabel('Actual')\n",
    "\n",
    "# Behavior analysis confusion matrix\n",
    "behavior_cm = confusion_matrix(y_behavior_test, behavior_predictions)\n",
    "sns.heatmap(behavior_cm, annot=True, fmt='d', cmap='Oranges', ax=axes[2])\n",
    "axes[2].set_title('Behavior Analysis - Confusion Matrix')\n",
    "axes[2].set_xlabel('Predicted')\n",
    "axes[2].set_ylabel('Actual')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/content/confusion_matrices.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Visualizations saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "download_section"
   },
   "source": [
    "## 8. Download Trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download_models"
   },
   "outputs": [],
   "source": [
    "# Create a summary of all trained models\n",
    "model_summary = {\n",
    "    \"training_date\": datetime.now().isoformat(),\n",
    "    \"models\": {\n",
    "        \"fire_detection\": {\n",
    "            \"type\": \"neural_network\",\n",
    "            \"accuracy\": float(fire_accuracy),\n",
    "            \"file\": \"fire_detection_model.h5\"\n",
    "        },\n",
    "        \"crowd_density\": {\n",
    "            \"type\": \"neural_network\",\n",
    "            \"accuracy\": float(crowd_accuracy),\n",
    "            \"file\": \"crowd_density_model.h5\"\n",
    "        },\n",
    "        \"behavior_analysis\": {\n",
    "            \"type\": \"random_forest\",\n",
    "            \"accuracy\": float(behavior_accuracy),\n",
    "            \"files\": [\n",
    "                \"behavior_analysis_model.pkl\",\n",
    "                \"behavior_label_encoder.pkl\",\n",
    "                \"behavior_scaler.pkl\"\n",
    "            ]\n",
    "        },\n",
    "        \"anomaly_detection\": {\n",
    "            \"type\": \"isolation_forest\",\n",
    "            \"sensor_types\": sensor_types,\n",
    "            \"files\": [f\"{sensor}_anomaly_model.pkl\" for sensor in sensor_types] + \n",
    "                     [f\"{sensor}_scaler.pkl\" for sensor in sensor_types]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save model summary\n",
    "with open('/content/model_summary.json', 'w') as f:\n",
    "    json.dump(model_summary, f, indent=2)\n",
    "\n",
    "# List all files to download\n",
    "files_to_download = [\n",
    "    '/content/fire_detection_model.h5',\n",
    "    '/content/crowd_density_model.h5',\n",
    "    '/content/behavior_analysis_model.pkl',\n",
    "    '/content/behavior_label_encoder.pkl',\n",
    "    '/content/behavior_scaler.pkl',\n",
    "    '/content/model_summary.json',\n",
    "    '/content/model_training_history.png',\n",
    "    '/content/confusion_matrices.png'\n",
    "]\n",
    "\n",
    "# Add anomaly detection files\n",
    "for sensor_type in sensor_types:\n",
    "    files_to_download.extend([\n",
    "        f'/content/{sensor_type}_anomaly_model.pkl',\n",
    "        f'/content/{sensor_type}_scaler.pkl'\n",
    "    ])\n",
    "\n",
    "print(\"Files ready for download:\")\n",
    "for file_path in files_to_download:\n",
    "    if os.path.exists(file_path):\n",
    "        print(f\"✓ {file_path}\")\n",
    "    else:\n",
    "        print(f\"✗ {file_path} (missing)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TRAINING COMPLETE!\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Fire Detection Accuracy: {fire_accuracy:.4f}\")\n",
    "print(f\"Crowd Density Accuracy: {crowd_accuracy:.4f}\")\n",
    "print(f\"Behavior Analysis Accuracy: {behavior_accuracy:.4f}\")\n",
    "print(\"\\nDownload all files and place them in your project's 'data/models/' directory.\")\n",
    "print(\"Update the model paths in your configuration files accordingly.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
