{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emergency Detection Model Training - Simplified\n",
    "## Large Events Emergency Management System\n",
    "\n",
    "This notebook trains ML models for detecting emergencies using Google Colab's GPU resources.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install tensorflow scikit-learn pandas numpy matplotlib seaborn opencv-python pillow joblib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# TensorFlow/Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, IsolationForest\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import joblib\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU available: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic training data\n",
    "def generate_fire_data(num_samples=2000):\n",
    "    \"\"\"Generate synthetic fire detection data\"\"\"\n",
    "    data = []\n",
    "    labels = []\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        if i < num_samples // 2:\n",
    "            # Fire images - higher red/orange values\n",
    "            red_intensity = np.random.normal(200, 30)\n",
    "            orange_intensity = np.random.normal(180, 25)\n",
    "            brightness = np.random.normal(150, 20)\n",
    "            motion_intensity = np.random.normal(80, 15)\n",
    "            label = 1  # Fire\n",
    "        else:\n",
    "            # Non-fire images\n",
    "            red_intensity = np.random.normal(100, 40)\n",
    "            orange_intensity = np.random.normal(90, 35)\n",
    "            brightness = np.random.normal(120, 30)\n",
    "            motion_intensity = np.random.normal(30, 20)\n",
    "            label = 0  # No fire\n",
    "        \n",
    "        features = [red_intensity, orange_intensity, brightness, motion_intensity,\n",
    "                   np.random.normal(50, 10), np.random.normal(75, 15)]\n",
    "        data.append(features)\n",
    "        labels.append(label)\n",
    "    \n",
    "    return np.array(data), np.array(labels)\n",
    "\n",
    "def generate_crowd_data(num_samples=2000):\n",
    "    \"\"\"Generate synthetic crowd density data\"\"\"\n",
    "    data = []\n",
    "    labels = []\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        people_count = np.random.randint(0, 200)\n",
    "        area_coverage = np.random.uniform(0.1, 0.9)\n",
    "        movement_speed = np.random.uniform(0, 5)\n",
    "        noise_level = np.random.uniform(40, 100)\n",
    "        \n",
    "        density = people_count * area_coverage / 100\n",
    "        if density < 1.0:\n",
    "            density_level = 0  # Low\n",
    "        elif density < 2.5:\n",
    "            density_level = 1  # Medium\n",
    "        elif density < 4.0:\n",
    "            density_level = 2  # High\n",
    "        else:\n",
    "            density_level = 3  # Critical\n",
    "        \n",
    "        features = [people_count, area_coverage, movement_speed, noise_level, density]\n",
    "        data.append(features)\n",
    "        labels.append(density_level)\n",
    "    \n",
    "    return np.array(data), np.array(labels)\n",
    "\n",
    "def generate_behavior_data(num_samples=2000):\n",
    "    \"\"\"Generate synthetic behavior analysis data\"\"\"\n",
    "    data = []\n",
    "    labels = []\n",
    "    \n",
    "    behavior_types = ['normal', 'suspicious', 'aggressive', 'panic']\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        behavior = np.random.choice(behavior_types)\n",
    "        \n",
    "        if behavior == 'normal':\n",
    "            motion_variance = np.random.normal(10, 3)\n",
    "            audio_level = np.random.normal(60, 10)\n",
    "            speed_changes = np.random.normal(2, 1)\n",
    "        elif behavior == 'suspicious':\n",
    "            motion_variance = np.random.normal(25, 5)\n",
    "            audio_level = np.random.normal(50, 8)\n",
    "            speed_changes = np.random.normal(8, 2)\n",
    "        elif behavior == 'aggressive':\n",
    "            motion_variance = np.random.normal(40, 8)\n",
    "            audio_level = np.random.normal(85, 15)\n",
    "            speed_changes = np.random.normal(15, 3)\n",
    "        else:  # panic\n",
    "            motion_variance = np.random.normal(60, 10)\n",
    "            audio_level = np.random.normal(95, 20)\n",
    "            speed_changes = np.random.normal(25, 5)\n",
    "        \n",
    "        features = [motion_variance, audio_level, speed_changes,\n",
    "                   np.random.normal(30, 10), np.random.normal(45, 12)]\n",
    "        data.append(features)\n",
    "        labels.append(behavior)\n",
    "    \n",
    "    return np.array(data), np.array(labels)\n",
    "\n",
    "# Generate datasets\n",
    "print(\"Generating synthetic training data...\")\n",
    "fire_data, fire_labels = generate_fire_data(2000)\n",
    "crowd_data, crowd_labels = generate_crowd_data(2000)\n",
    "behavior_data, behavior_labels = generate_behavior_data(2000)\n",
    "\n",
    "print(f\"Fire detection data: {fire_data.shape}\")\n",
    "print(f\"Crowd analysis data: {crowd_data.shape}\")\n",
    "print(f\"Behavior analysis data: {behavior_data.shape}\")\n",
    "print(\"âœ… Data generation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Fire Detection Model\n",
    "print(\"ðŸ”¥ Training Fire Detection Model...\")\n",
    "\n",
    "X_fire_train, X_fire_test, y_fire_train, y_fire_test = train_test_split(\n",
    "    fire_data, fire_labels, test_size=0.2, random_state=42, stratify=fire_labels\n",
    ")\n",
    "\n",
    "fire_scaler = StandardScaler()\n",
    "X_fire_train_scaled = fire_scaler.fit_transform(X_fire_train)\n",
    "X_fire_test_scaled = fire_scaler.transform(X_fire_test)\n",
    "\n",
    "fire_model = models.Sequential([\n",
    "    layers.Dense(128, activation='relu', input_shape=(X_fire_train_scaled.shape[1],)),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "fire_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "fire_history = fire_model.fit(\n",
    "    X_fire_train_scaled, y_fire_train,\n",
    "    validation_data=(X_fire_test_scaled, y_fire_test),\n",
    "    epochs=30, batch_size=32, verbose=1\n",
    ")\n",
    "\n",
    "fire_predictions = (fire_model.predict(X_fire_test_scaled) > 0.5).astype(int)\n",
    "fire_accuracy = accuracy_score(y_fire_test, fire_predictions)\n",
    "\n",
    "print(f\"ðŸ”¥ Fire Detection Accuracy: {fire_accuracy:.4f}\")\n",
    "fire_model.save('/content/fire_detection_model.h5')\n",
    "joblib.dump(fire_scaler, '/content/fire_scaler.pkl')\n",
    "print(\"âœ… Fire detection model saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Crowd Density Model\n",
    "print(\"ðŸ‘¥ Training Crowd Density Model...\")\n",
    "\n",
    "X_crowd_train, X_crowd_test, y_crowd_train, y_crowd_test = train_test_split(\n",
    "    crowd_data, crowd_labels, test_size=0.2, random_state=42, stratify=crowd_labels\n",
    ")\n",
    "\n",
    "crowd_scaler = StandardScaler()\n",
    "X_crowd_train_scaled = crowd_scaler.fit_transform(X_crowd_train)\n",
    "X_crowd_test_scaled = crowd_scaler.transform(X_crowd_test)\n",
    "\n",
    "num_classes = len(np.unique(crowd_labels))\n",
    "crowd_model = models.Sequential([\n",
    "    layers.Dense(128, activation='relu', input_shape=(X_crowd_train_scaled.shape[1],)),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "crowd_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "crowd_history = crowd_model.fit(\n",
    "    X_crowd_train_scaled, y_crowd_train,\n",
    "    validation_data=(X_crowd_test_scaled, y_crowd_test),\n",
    "    epochs=30, batch_size=32, verbose=1\n",
    ")\n",
    "\n",
    "crowd_predictions = np.argmax(crowd_model.predict(X_crowd_test_scaled), axis=1)\n",
    "crowd_accuracy = accuracy_score(y_crowd_test, crowd_predictions)\n",
    "\n",
    "print(f\"ðŸ‘¥ Crowd Density Accuracy: {crowd_accuracy:.4f}\")\n",
    "crowd_model.save('/content/crowd_density_model.h5')\n",
    "joblib.dump(crowd_scaler, '/content/crowd_scaler.pkl')\n",
    "print(\"âœ… Crowd density model saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Behavior Analysis Model\n",
    "print(\"ðŸš¨ Training Behavior Analysis Model...\")\n",
    "\n",
    "behavior_encoder = LabelEncoder()\n",
    "behavior_labels_encoded = behavior_encoder.fit_transform(behavior_labels)\n",
    "\n",
    "X_behavior_train, X_behavior_test, y_behavior_train, y_behavior_test = train_test_split(\n",
    "    behavior_data, behavior_labels_encoded, test_size=0.2, random_state=42, stratify=behavior_labels_encoded\n",
    ")\n",
    "\n",
    "behavior_scaler = StandardScaler()\n",
    "X_behavior_train_scaled = behavior_scaler.fit_transform(X_behavior_train)\n",
    "X_behavior_test_scaled = behavior_scaler.transform(X_behavior_test)\n",
    "\n",
    "behavior_model = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "behavior_model.fit(X_behavior_train_scaled, y_behavior_train)\n",
    "\n",
    "behavior_predictions = behavior_model.predict(X_behavior_test_scaled)\n",
    "behavior_accuracy = accuracy_score(y_behavior_test, behavior_predictions)\n",
    "\n",
    "print(f\"ðŸš¨ Behavior Analysis Accuracy: {behavior_accuracy:.4f}\")\n",
    "joblib.dump(behavior_model, '/content/behavior_analysis_model.pkl')\n",
    "joblib.dump(behavior_encoder, '/content/behavior_label_encoder.pkl')\n",
    "joblib.dump(behavior_scaler, '/content/behavior_scaler.pkl')\n",
    "print(\"âœ… Behavior analysis model saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Sensor Anomaly Detection Models\n",
    "print(\"ðŸ“Š Training Sensor Anomaly Detection Models...\")\n",
    "\n",
    "def generate_sensor_data(num_samples=2000):\n",
    "    data = []\n",
    "    for i in range(num_samples):\n",
    "        if i < num_samples * 0.9:  # Normal readings\n",
    "            temperature = np.random.normal(22, 3)\n",
    "            humidity = np.random.normal(45, 8)\n",
    "            sound = np.random.normal(65, 10)\n",
    "            motion = np.random.normal(20, 5)\n",
    "        else:  # Anomalous readings\n",
    "            temperature = np.random.choice([np.random.normal(45, 5), np.random.normal(-5, 2)])\n",
    "            humidity = np.random.normal(85, 10)\n",
    "            sound = np.random.normal(110, 15)\n",
    "            motion = np.random.normal(80, 20)\n",
    "        data.append([temperature, humidity, sound, motion])\n",
    "    return np.array(data)\n",
    "\n",
    "sensor_data = generate_sensor_data(2000)\n",
    "sensor_types = ['temperature', 'humidity', 'sound', 'motion']\n",
    "\n",
    "for i, sensor_type in enumerate(sensor_types):\n",
    "    print(f\"Training {sensor_type} anomaly detector...\")\n",
    "    \n",
    "    sensor_values = sensor_data[:, i].reshape(-1, 1)\n",
    "    normal_data = sensor_values[:int(len(sensor_values) * 0.9)]\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    normal_data_scaled = scaler.fit_transform(normal_data)\n",
    "    \n",
    "    anomaly_model = IsolationForest(contamination=0.1, random_state=42)\n",
    "    anomaly_model.fit(normal_data_scaled)\n",
    "    \n",
    "    # Test accuracy\n",
    "    all_data_scaled = scaler.transform(sensor_values)\n",
    "    predictions = anomaly_model.predict(all_data_scaled)\n",
    "    \n",
    "    joblib.dump(anomaly_model, f'/content/{sensor_type}_anomaly_model.pkl')\n",
    "    joblib.dump(scaler, f'/content/{sensor_type}_scaler.pkl')\n",
    "    \n",
    "    print(f\"âœ… {sensor_type} anomaly detector saved!\")\n",
    "\n",
    "print(\"ðŸ“Š All sensor anomaly models trained!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model summary and visualizations\n",
    "model_summary = {\n",
    "    \"training_date\": datetime.now().isoformat(),\n",
    "    \"models\": {\n",
    "        \"fire_detection\": {\n",
    "            \"type\": \"neural_network\",\n",
    "            \"accuracy\": float(fire_accuracy),\n",
    "            \"file\": \"fire_detection_model.h5\"\n",
    "        },\n",
    "        \"crowd_density\": {\n",
    "            \"type\": \"neural_network\",\n",
    "            \"accuracy\": float(crowd_accuracy),\n",
    "            \"file\": \"crowd_density_model.h5\"\n",
    "        },\n",
    "        \"behavior_analysis\": {\n",
    "            \"type\": \"random_forest\",\n",
    "            \"accuracy\": float(behavior_accuracy),\n",
    "            \"files\": [\"behavior_analysis_model.pkl\", \"behavior_label_encoder.pkl\", \"behavior_scaler.pkl\"]\n",
    "        },\n",
    "        \"anomaly_detection\": {\n",
    "            \"type\": \"isolation_forest\",\n",
    "            \"sensor_types\": sensor_types,\n",
    "            \"files\": [f\"{sensor}_anomaly_model.pkl\" for sensor in sensor_types] + [f\"{sensor}_scaler.pkl\" for sensor in sensor_types]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('/content/model_summary.json', 'w') as f:\n",
    "    json.dump(model_summary, f, indent=2)\n",
    "\n",
    "# Plot training results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Fire model\n",
    "axes[0, 0].plot(fire_history.history['accuracy'], label='Training')\n",
    "axes[0, 0].plot(fire_history.history['val_accuracy'], label='Validation')\n",
    "axes[0, 0].set_title('Fire Detection - Accuracy')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "axes[0, 1].plot(fire_history.history['loss'], label='Training')\n",
    "axes[0, 1].plot(fire_history.history['val_loss'], label='Validation')\n",
    "axes[0, 1].set_title('Fire Detection - Loss')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# Crowd model\n",
    "axes[1, 0].plot(crowd_history.history['accuracy'], label='Training')\n",
    "axes[1, 0].plot(crowd_history.history['val_accuracy'], label='Validation')\n",
    "axes[1, 0].set_title('Crowd Density - Accuracy')\n",
    "axes[1, 0].legend()\n",
    "\n",
    "axes[1, 1].plot(crowd_history.history['loss'], label='Training')\n",
    "axes[1, 1].plot(crowd_history.history['val_loss'], label='Validation')\n",
    "axes[1, 1].set_title('Crowd Density - Loss')\n",
    "axes[1, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/content/training_results.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ðŸŽ‰ TRAINING COMPLETE!\")\n",
    "print(\"=\"*50)\n",
    "print(f\"ðŸ”¥ Fire Detection Accuracy: {fire_accuracy:.4f}\")\n",
    "print(f\"ðŸ‘¥ Crowd Density Accuracy: {crowd_accuracy:.4f}\")\n",
    "print(f\"ðŸš¨ Behavior Analysis Accuracy: {behavior_accuracy:.4f}\")\n",
    "print(\"\\nðŸ“ Files ready for download:\")\n",
    "print(\"- fire_detection_model.h5\")\n",
    "print(\"- crowd_density_model.h5\")\n",
    "print(\"- behavior_analysis_model.pkl\")\n",
    "print(\"- behavior_label_encoder.pkl\")\n",
    "print(\"- behavior_scaler.pkl\")\n",
    "print(\"- fire_scaler.pkl\")\n",
    "print(\"- crowd_scaler.pkl\")\n",
    "for sensor in sensor_types:\n",
    "    print(f\"- {sensor}_anomaly_model.pkl\")\n",
    "    print(f\"- {sensor}_scaler.pkl\")\n",
    "print(\"- model_summary.json\")\n",
    "print(\"- training_results.png\")\n",
    "print(\"\\nâœ… Download all files and place them in your project's 'data/models/' directory.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
